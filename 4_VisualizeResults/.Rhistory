else if (ue %in% bottom_experiments){
plot(cv_preds[cv_preds$experiment == ue & cv_preds$algo == algo,]$tr,
cv_preds[cv_preds$experiment == ue & cv_preds$algo == algo,]$pred,
pch=point_type,col=col_points[point_type],cex=0.75,
xlim=c(0,max(cv_preds[cv_preds$experiment == ue,]$tr*1.1)),
ylim=c(0,max(cv_preds[cv_preds$experiment == ue,]$tr*1.1)),
xlab="",ylab=""
)
mtext("Experimental retention time (s)",side=1,line=2,cex=0.6)
}
else {
plot(cv_preds[cv_preds$experiment == ue & cv_preds$algo == algo,]$tr,
cv_preds[cv_preds$experiment == ue & cv_preds$algo == algo,]$pred,
pch=point_type,col=col_points[point_type],cex=0.75,
xlim=c(0,max(cv_preds[cv_preds$experiment == ue,]$tr*1.1)),
ylim=c(0,max(cv_preds[cv_preds$experiment == ue,]$tr*1.1)),
xlab="",ylab=""
)
}
text(x = max(cv_preds[cv_preds$experiment == ue,]$tr)/2, y = max(cv_preds[cv_preds$experiment == ue,]$tr), labels = algo)
abline(a=0,b=1)
if (algo == "AB"){
title(paste(ue))
}
#ue," - ",algo
}
}
plot_eps <- T
#########################################################################################
#                                                                                       #
# Plot the CV results - big feature set                                                 #
#                                                                                       #
#########################################################################################
#######################################################################
#                                                                     #
# Make a pairwise comparison between XGBoost and the other algorithms #
#                                                                     #
#######################################################################
# Read the CV results for the 151 feature set
cv_preds <- read.csv("data/predictions_algo_verbose_big_ann_cv.csv")
# Get the experiments from the CV set
unique_experi <- unique(cv_preds$experiment)
# Get the algos from the CV set
unique_algo <- unique(cv_preds$algo)
# Initialize vectors that will hold the performance measures per dataset and algorithm
t_algo_cor <- c()
t_algo_mae <- c()
t_algo_me <- c()
# Iterate over the experiments and algorithms and use different performance measures
for (e in unique_experi){
for (a in unique_algo){
temp_cv_preds <- cv_preds[cv_preds$experiment == e & cv_preds$algo == a,]
t_algo_mae <- c(t_algo_mae,sum(abs(temp_cv_preds$tr-temp_cv_preds$pred),na.rm=T)/length(temp_cv_preds$tr))
t_algo_cor <- c(t_algo_cor,cor(temp_cv_preds$tr,temp_cv_preds$pred))
t_algo_me <- c(t_algo_me,median(abs(temp_cv_preds$tr-temp_cv_preds$pred),na.rm=T))
}
}
# Make sure the performance measures are put in a matrix with the correct algorithm and experiment name
perf_mae <- data.frame(matrix(t_algo_mae,ncol=length(unique_algo),byrow=T))
colnames(perf_mae) <- unique_algo
rownames(perf_mae) <- unique_experi
perf_me <- data.frame(matrix(t_algo_me,ncol=length(unique_algo),byrow=T))
colnames(perf_me) <- unique_algo
rownames(perf_me) <- unique_experi
# One plot per plotting window
par(mfrow=c(1,1))
# Plot eps?
if (plot_eps){
postscript(paste0(output_dir,"pairwise_comparison_ann_depth.eps"), width = 14, height = 4.2, horizontal = FALSE,
onefile = FALSE, paper = "special", colormodel = "cmyk",
family = "Times")
}
# Plot settings
par(mfrow=c(1,3))
par(mar=c(5, 4, 0.5, 2) + 0.1)
# Vector that holds combinations that have already been analyzed (counters A -> B and B -> A comparisons)
analyzed <- c()
# Get the maximum retention time per experiment based on the last identification
max_rt <- tapply(cv_preds$tr, cv_preds$experiment, max)[rownames(perf_me)]
# Iterate over all combinations of algorithms and do a pairwise comparison
for (a in c("ANN - 2 hidden layers","ANN - 1 hidden layer","ANN - 5 hidden layers")){
for (b in c("ANN - 2 hidden layers","ANN - 1 hidden layer","ANN - 5 hidden layers")){
# If not analyzed yet; make a pairwise comparison between the algorithms
if (a != b & !(paste(a,b,sep="_") %in% analyzed) & !(paste(b,a,sep="_") %in% analyzed)){
# Change margins for algorithms at the sides...
par(mar=c(8, 5, 4, 2) + 0.1)
# Calculate the median error and normalized median error
perf_me$diff_algo <- perf_me[,b]-perf_me[,a]
perf_me$diff_algo_order <- (perf_me[,b]-perf_me[,a])/max_rt
perf <- (perf_me$diff_algo[order(perf_me$diff_algo_order)]/max_rt[order(perf_me$diff_algo_order)])*100
# Plot the pairwise comparison
barplot(perf,
names=rownames(perf_me)[order(perf_me$diff_algo)],las=2,cex.names=0.7,
main=paste(b," - ",a,"\n Average increase (%):",round(mean(perf),3)),
ylim=c(-10,10),ylab="Difference median error relative\n to the total elution time (%)")
# Add pair to analyzed combinations
analyzed <- c(analyzed,paste(a,b,sep="_"))
}
}
}
# Close dev if figure was plotted
if (plot_eps){
dev.off()
}
#########################################################################################
#                                                                                       #
# Plot the CV results - big feature set                                                 #
#                                                                                       #
#########################################################################################
#######################################################################
#                                                                     #
# Make a pairwise comparison between XGBoost and the other algorithms #
#                                                                     #
#######################################################################
# Read the CV results for the 151 feature set
cv_preds <- read.csv("data/predictions_algo_verbose_big_svr_cv.csv")
# Get the experiments from the CV set
unique_experi <- unique(cv_preds$experiment)
# Get the algos from the CV set
unique_algo <- unique(cv_preds$algo)
# Initialize vectors that will hold the performance measures per dataset and algorithm
t_algo_cor <- c()
t_algo_mae <- c()
t_algo_me <- c()
# Iterate over the experiments and algorithms and use different performance measures
for (e in unique_experi){
for (a in unique_algo){
temp_cv_preds <- cv_preds[cv_preds$experiment == e & cv_preds$algo == a,]
t_algo_mae <- c(t_algo_mae,sum(abs(temp_cv_preds$tr-temp_cv_preds$pred),na.rm=T)/length(temp_cv_preds$tr))
t_algo_cor <- c(t_algo_cor,cor(temp_cv_preds$tr,temp_cv_preds$pred))
t_algo_me <- c(t_algo_me,median(abs(temp_cv_preds$tr-temp_cv_preds$pred),na.rm=T))
}
}
# Make sure the performance measures are put in a matrix with the correct algorithm and experiment name
perf_mae <- data.frame(matrix(t_algo_mae,ncol=length(unique_algo),byrow=T))
colnames(perf_mae) <- unique_algo
rownames(perf_mae) <- unique_experi
perf_me <- data.frame(matrix(t_algo_me,ncol=length(unique_algo),byrow=T))
colnames(perf_me) <- unique_algo
rownames(perf_me) <- unique_experi
# One plot per plotting window
par(mfrow=c(1,1))
# Plot eps?
if (plot_eps){
postscript(paste0(output_dir,"pairwise_comparison_svr_kernel.eps"), width = 14, height = 4.2, horizontal = FALSE,
onefile = FALSE, paper = "special", colormodel = "cmyk",
family = "Times")
}
# Plot settings
par(mfrow=c(1,3))
par(mar=c(5, 4, 0.5, 2) + 0.1)
# Vector that holds combinations that have already been analyzed (counters A -> B and B -> A comparisons)
analyzed <- c()
# Get the maximum retention time per experiment based on the last identification
max_rt <- tapply(cv_preds$tr, cv_preds$experiment, max)[rownames(perf_me)]
# Iterate over all combinations of algorithms and do a pairwise comparison
for (a in c("SVR","SVR-L","SVR-RBF")){
for (b in c("SVR","SVR-L","SVR-RBF")){
# If not analyzed yet; make a pairwise comparison between the algorithms
if (a != b & !(paste(a,b,sep="_") %in% analyzed) & !(paste(b,a,sep="_") %in% analyzed)){
# Change margins for algorithms at the sides...
par(mar=c(8, 5, 4, 2) + 0.1)
# Calculate the median error and normalized median error
perf_me$diff_algo <- perf_me[,b]-perf_me[,a]
perf_me$diff_algo_order <- (perf_me[,b]-perf_me[,a])/max_rt
perf <- (perf_me$diff_algo[order(perf_me$diff_algo_order)]/max_rt[order(perf_me$diff_algo_order)])*100
# Plot the pairwise comparison
barplot(perf,
names=rownames(perf_me)[order(perf_me$diff_algo)],las=2,cex.names=0.7,
main=paste(b," - ",a,"\n Average increase (%):",round(mean(perf),3)),
ylim=c(-10,10),ylab="Difference median error relative\n to the total elution time (%)")
# Add pair to analyzed combinations
analyzed <- c(analyzed,paste(a,b,sep="_"))
}
}
}
# Close dev if figure was plotted
if (plot_eps){
dev.off()
}
#########################################################################################
#                                                                                       #
# Plot the CV results - big feature set                                                 #
#                                                                                       #
#########################################################################################
#######################################################################
#                                                                     #
# Make a pairwise comparison between XGBoost and the other algorithms #
#                                                                     #
#######################################################################
# Read the CV results for the 151 feature set
cv_preds <- read.csv("data/predictions_algo_verbose_simple_cv.csv")
# Get the experiments from the CV set
unique_experi <- unique(cv_preds$experiment)
# Get the algos from the CV set
unique_algo <- unique(cv_preds$algo)
# Initialize vectors that will hold the performance measures per dataset and algorithm
t_algo_cor <- c()
t_algo_mae <- c()
t_algo_me <- c()
# Iterate over the experiments and algorithms and use different performance measures
for (e in unique_experi){
for (a in unique_algo){
temp_cv_preds <- cv_preds[cv_preds$experiment == e & cv_preds$algo == a,]
t_algo_mae <- c(t_algo_mae,sum(abs(temp_cv_preds$tr-temp_cv_preds$pred),na.rm=T)/length(temp_cv_preds$tr))
t_algo_cor <- c(t_algo_cor,cor(temp_cv_preds$tr,temp_cv_preds$pred))
t_algo_me <- c(t_algo_me,median(abs(temp_cv_preds$tr-temp_cv_preds$pred),na.rm=T))
}
}
# Make sure the performance measures are put in a matrix with the correct algorithm and experiment name
perf_mae <- data.frame(matrix(t_algo_mae,ncol=length(unique_algo),byrow=T))
colnames(perf_mae) <- unique_algo
rownames(perf_mae) <- unique_experi
perf_me <- data.frame(matrix(t_algo_me,ncol=length(unique_algo),byrow=T))
colnames(perf_me) <- unique_algo
rownames(perf_me) <- unique_experi
# One plot per plotting window
par(mfrow=c(1,1))
# Plot eps?
if (plot_eps){
postscript(paste0(output_dir,"pairwise_comparison_simple.eps"), width = 11.5, height = 12, horizontal = FALSE,
onefile = FALSE, paper = "special", colormodel = "cmyk",
family = "Times")
}
# Plot settings
par(mfrow=c(4,2))
par(mar=c(5, 4, 0.5, 2) + 0.1)
# Vector that holds combinations that have already been analyzed (counters A -> B and B -> A comparisons)
analyzed <- c()
# Get the maximum retention time per experiment based on the last identification
max_rt <- tapply(cv_preds$tr, cv_preds$experiment, max)[rownames(perf_me)]
# Iterate over all combinations of algorithms and do a pairwise comparison
for (a in c("MolLogP")){
for (b in c("SVR","GB","BRR","AB","RF","ANN", "GB","LASSO")){
# If not analyzed yet; make a pairwise comparison between the algorithms
if (a != b & !(paste(a,b,sep="_") %in% analyzed) & !(paste(b,a,sep="_") %in% analyzed)){
# Change margins for algorithms at the sides...
par(mar=c(8,5, 4, 2) + 0.1)
# Calculate the median error and normalized median error
perf_me$diff_algo <- perf_me[,b]-perf_me[,a]
perf_me$diff_algo_order <- (perf_me[,b]-perf_me[,a])/max_rt
perf <- (perf_me$diff_algo[order(perf_me$diff_algo_order)]/max_rt[order(perf_me$diff_algo_order)])*100
# Plot the pairwise comparison
barplot(perf,
names=rownames(perf_me)[order(perf_me$diff_algo)],las=2,cex.names=0.7,
main=paste(b," - ",a,"\n Average increase (%):",round(mean(perf),3)),
ylim=c(-10,10),ylab="Difference median error relative\n to the total elution time (%)")
# Add pair to analyzed combinations
analyzed <- c(analyzed,paste(a,b,sep="_"))
}
}
}
# Close dev if figure was plotted
if (plot_eps){
dev.off()
}
#########################################################################################
#                                                                                       #
# Plot the CV results - big feature set                                                 #
#                                                                                       #
#########################################################################################
#######################################################################
#                                                                     #
# Make a pairwise comparison between XGBoost and the other algorithms #
#                                                                     #
#######################################################################
# Read the CV results for the 151 feature set
cv_preds <- read.csv("data/predictions_algo_verbose_simple_cv.csv")
# Get the experiments from the CV set
unique_experi <- unique(cv_preds$experiment)
# Get the algos from the CV set
unique_algo <- unique(cv_preds$algo)
# Initialize vectors that will hold the performance measures per dataset and algorithm
t_algo_cor <- c()
t_algo_mae <- c()
t_algo_me <- c()
# Iterate over the experiments and algorithms and use different performance measures
for (e in unique_experi){
for (a in unique_algo){
temp_cv_preds <- cv_preds[cv_preds$experiment == e & cv_preds$algo == a,]
t_algo_mae <- c(t_algo_mae,sum(abs(temp_cv_preds$tr-temp_cv_preds$pred),na.rm=T)/length(temp_cv_preds$tr))
t_algo_cor <- c(t_algo_cor,cor(temp_cv_preds$tr,temp_cv_preds$pred))
t_algo_me <- c(t_algo_me,median(abs(temp_cv_preds$tr-temp_cv_preds$pred),na.rm=T))
}
}
# Make sure the performance measures are put in a matrix with the correct algorithm and experiment name
perf_mae <- data.frame(matrix(t_algo_mae,ncol=length(unique_algo),byrow=T))
colnames(perf_mae) <- unique_algo
rownames(perf_mae) <- unique_experi
perf_me <- data.frame(matrix(t_algo_me,ncol=length(unique_algo),byrow=T))
colnames(perf_me) <- unique_algo
rownames(perf_me) <- unique_experi
# One plot per plotting window
par(mfrow=c(1,1))
# Plot eps?
if (plot_eps){
postscript(paste0(output_dir,"pairwise_comparison_simple_zero.eps"), width = 11.5, height = 12, horizontal = FALSE,
onefile = FALSE, paper = "special", colormodel = "cmyk",
family = "Times")
}
# Plot settings
par(mfrow=c(4,2))
par(mar=c(5, 4, 0.5, 2) + 0.1)
# Vector that holds combinations that have already been analyzed (counters A -> B and B -> A comparisons)
analyzed <- c()
# Get the maximum retention time per experiment based on the last identification
max_rt <- tapply(cv_preds$tr, cv_preds$experiment, max)[rownames(perf_me)]
# Iterate over all combinations of algorithms and do a pairwise comparison
for (a in c("MolLogP")){
for (b in c("SVR","GB","BRR","AB","RF","ANN", "GB","LASSO")){
# If not analyzed yet; make a pairwise comparison between the algorithms
if (a != b & !(paste(a,b,sep="_") %in% analyzed) & !(paste(b,a,sep="_") %in% analyzed)){
# Change margins for algorithms at the sides...
par(mar=c(8,5, 4, 2) + 0.1)
# Calculate the median error and normalized median error
perf_me$diff_algo <- perf_me[,b]-max_rt/2 #perf_me[,a]
perf_me$diff_algo_order <- (perf_me[,b]-(max_rt/2))/max_rt
perf <- (perf_me$diff_algo[order(perf_me$diff_algo_order)]/max_rt[order(perf_me$diff_algo_order)])*100
# Plot the pairwise comparison
barplot(perf,
names=rownames(perf_me)[order(perf_me$diff_algo)],las=2,cex.names=0.7,
main=paste(b," - ",a,"\n Average increase (%):",round(mean(perf),3)),
ylim=c(-55,0),ylab="Difference median error relative\n to the total elution time (%)")
# Add pair to analyzed combinations
analyzed <- c(analyzed,paste(a,b,sep="_"))
}
}
}
# Close dev if figure was plotted
if (plot_eps){
dev.off()
}
#########################################################################################
#                                                                                       #
# Plot the CV results - big feature set                                                 #
#                                                                                       #
#########################################################################################
#######################################################################
#                                                                     #
# Make a pairwise comparison between XGBoost and the other algorithms #
#                                                                     #
#######################################################################
# Read the CV results for the 151 feature set
cv_preds <- read.csv("data/predictions_algo_verbose_big_cv_simple_molwt.csv")
# Get the experiments from the CV set
unique_experi <- unique(cv_preds$experiment)
# Get the algos from the CV set
unique_algo <- unique(cv_preds$algo)
# Initialize vectors that will hold the performance measures per dataset and algorithm
t_algo_cor <- c()
t_algo_mae <- c()
t_algo_me <- c()
# Iterate over the experiments and algorithms and use different performance measures
for (e in unique_experi){
for (a in unique_algo){
temp_cv_preds <- cv_preds[cv_preds$experiment == e & cv_preds$algo == a,]
t_algo_mae <- c(t_algo_mae,sum(abs(temp_cv_preds$tr-temp_cv_preds$pred),na.rm=T)/length(temp_cv_preds$tr))
t_algo_cor <- c(t_algo_cor,cor(temp_cv_preds$tr,temp_cv_preds$pred))
t_algo_me <- c(t_algo_me,median(abs(temp_cv_preds$tr-temp_cv_preds$pred),na.rm=T))
}
}
# Make sure the performance measures are put in a matrix with the correct algorithm and experiment name
perf_mae <- data.frame(matrix(t_algo_mae,ncol=length(unique_algo),byrow=T))
colnames(perf_mae) <- unique_algo
rownames(perf_mae) <- unique_experi
perf_me <- data.frame(matrix(t_algo_me,ncol=length(unique_algo),byrow=T))
colnames(perf_me) <- unique_algo
rownames(perf_me) <- unique_experi
# One plot per plotting window
par(mfrow=c(1,1))
# Plot eps?
if (plot_eps){
postscript(paste0(output_dir,"pairwise_comparison_simple.eps"), width = 11.5, height = 12, horizontal = FALSE,
onefile = FALSE, paper = "special", colormodel = "cmyk",
family = "Times")
}
# Plot settings
par(mfrow=c(4,2))
par(mar=c(5, 4, 0.5, 2) + 0.1)
# Vector that holds combinations that have already been analyzed (counters A -> B and B -> A comparisons)
analyzed <- c()
# Get the maximum retention time per experiment based on the last identification
max_rt <- tapply(cv_preds$tr, cv_preds$experiment, max)[rownames(perf_me)]
# Iterate over all combinations of algorithms and do a pairwise comparison
for (a in c("MolWT")){
for (b in c("SVR","GB","BRR","AB","RF","ANN", "GB","LASSO")){
# If not analyzed yet; make a pairwise comparison between the algorithms
if (a != b & !(paste(a,b,sep="_") %in% analyzed) & !(paste(b,a,sep="_") %in% analyzed)){
# Change margins for algorithms at the sides...
par(mar=c(8,5, 4, 2) + 0.1)
# Calculate the median error and normalized median error
perf_me$diff_algo <- perf_me[,b]-perf_me[,a]
perf_me$diff_algo_order <- (perf_me[,b]-perf_me[,a])/max_rt
perf <- (perf_me$diff_algo[order(perf_me$diff_algo_order)]/max_rt[order(perf_me$diff_algo_order)])*100
# Plot the pairwise comparison
barplot(perf,
names=rownames(perf_me)[order(perf_me$diff_algo)],las=2,cex.names=0.7,
main=paste(b," - ",a,"\n Average increase (%):",round(mean(perf),3)),
ylim=c(-10,10),ylab="Difference median error relative\n to the total elution time (%)")
# Add pair to analyzed combinations
analyzed <- c(analyzed,paste(a,b,sep="_"))
}
}
}
# Close dev if figure was plotted
if (plot_eps){
dev.off()
}
#########################################################################################
#                                                                                       #
# Plot the CV results - big feature set                                                 #
#                                                                                       #
#########################################################################################
#######################################################################
#                                                                     #
# Make a pairwise comparison between XGBoost and the other algorithms #
#                                                                     #
#######################################################################
# Read the CV results for the 151 feature set
cv_preds <- read.csv("data/predictions_algo_verbose_big_cv_optimizer.csv")
# Get the experiments from the CV set
unique_experi <- unique(cv_preds$experiment)
# Get the algos from the CV set
unique_algo <- unique(cv_preds$algo)
# Initialize vectors that will hold the performance measures per dataset and algorithm
t_algo_cor <- c()
t_algo_mae <- c()
t_algo_me <- c()
# Iterate over the experiments and algorithms and use different performance measures
for (e in unique_experi){
for (a in unique_algo){
temp_cv_preds <- cv_preds[cv_preds$experiment == e & cv_preds$algo == a,]
t_algo_mae <- c(t_algo_mae,sum(abs(temp_cv_preds$tr-temp_cv_preds$pred),na.rm=T)/length(temp_cv_preds$tr))
t_algo_cor <- c(t_algo_cor,cor(temp_cv_preds$tr,temp_cv_preds$pred))
t_algo_me <- c(t_algo_me,median(abs(temp_cv_preds$tr-temp_cv_preds$pred),na.rm=T))
}
}
# Make sure the performance measures are put in a matrix with the correct algorithm and experiment name
perf_mae <- data.frame(matrix(t_algo_mae,ncol=length(unique_algo),byrow=T))
colnames(perf_mae) <- unique_algo
rownames(perf_mae) <- unique_experi
perf_me <- data.frame(matrix(t_algo_me,ncol=length(unique_algo),byrow=T))
colnames(perf_me) <- unique_algo
rownames(perf_me) <- unique_experi
# One plot per plotting window
par(mfrow=c(1,1))
# Plot eps?
if (plot_eps){
postscript(paste0(output_dir,"pairwise_comparison_ann_optimizer.eps"), width = 11.5, height = 12, horizontal = FALSE,
onefile = FALSE, paper = "special", colormodel = "cmyk",
family = "Times")
}
# Plot settings
par(mfrow=c(4,2))
par(mar=c(5, 4, 0.5, 2) + 0.1)
# Vector that holds combinations that have already been analyzed (counters A -> B and B -> A comparisons)
analyzed <- c()
# Get the maximum retention time per experiment based on the last identification
max_rt <- tapply(cv_preds$tr, cv_preds$experiment, max)[rownames(perf_me)]
# Iterate over all combinations of algorithms and do a pairwise comparison
for (a in c("GSD","ADAM")){
for (b in c("GSD","ADAM")){
# If not analyzed yet; make a pairwise comparison between the algorithms
if (a != b & !(paste(a,b,sep="_") %in% analyzed) & !(paste(b,a,sep="_") %in% analyzed)){
# Change margins for algorithms at the sides...
par(mar=c(8, 2, 4, 2) + 0.1)
# Calculate the median error and normalized median error
perf_me$diff_algo <- perf_me[,b]-perf_me[,a]
perf_me$diff_algo_order <- (perf_me[,b]-perf_me[,a])/max_rt
perf <- (perf_me$diff_algo[order(perf_me$diff_algo_order)]/max_rt[order(perf_me$diff_algo_order)])*100
# Plot the pairwise comparison
barplot(perf,
names=rownames(perf_me)[order(perf_me$diff_algo)],las=2,cex.names=0.7,
main=paste(b," - ",a,"\n Average increase (%):",round(mean(perf),3)),
ylim=c(-10,10),ylab="Difference median error relative\n to the total elution time (%)")
# Add pair to analyzed combinations
analyzed <- c(analyzed,paste(a,b,sep="_"))
}
}
}
# Close dev if figure was plotted
if (plot_eps){
dev.off()
}
# Set a working directory if needed
#setwd("")
postscript("figs/mw_distribution.eps", width = 7, height = 7, horizontal = FALSE,
onefile = FALSE, paper = "special", colormodel = "cmyk",
family = "Times")
data <- read.csv("data/concat_df.csv")
hist(data$MolWt,breaks=200,xlab="Molecular weight (Da)",main="Distribution of molecular weight\n for the compiled dataset")
diff_data <- unique(data$system)
mat_d <- c()
for (d in diff_data) {
mat_d <- c(mat_d,c(d,length(data[data$system == d,]$MolWt),max(data[data$system == d,]$MolWt),min(data[data$system == d,]$MolWt),max(data[data$system == d,]$MolLogP),min(data[data$system == d,]$MolLogP)))
}
dev.off()
source('~/Documents/idp/tRPredictionOverview/4_VisualizeResults/plotMW.R')
# Set a working directory if needed
#setwd("")
postscript("figs/mw_distribution.eps", width = 7, height = 7, horizontal = FALSE,
onefile = FALSE, paper = "special", colormodel = "cmyk",
family = "Times")
data <- read.csv("data/concat_df.csv")
hist(data$MolWt,breaks=200,xlab="Molecular weight (Da)",main="Distribution of molecular weight\n for the compiled dataset")
diff_data <- unique(data$system)
mat_d <- c()
for (d in diff_data) {
mat_d <- c(mat_d,c(d,length(data[data$system == d,]$MolWt),max(data[data$system == d,]$MolWt),min(data[data$system == d,]$MolWt),max(data[data$system == d,]$MolLogP),min(data[data$system == d,]$MolLogP)))
}
dev.off()
q()
